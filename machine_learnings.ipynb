{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1f3b37",
   "metadata": {},
   "source": [
    "## Entrenamiento de modelos ML para detección de accesos atípicos (IAM)\n",
    "\n",
    "En este notebook se van a entrenar y evaluar diferentes modelos de detección de anomalías sobre el dataset generado en la fase de preparación. Dado que no existen etiquetas explícitas de anomalía en el dataset LANL, se van a aplicar técnicas no supervisadas / semi-supervisadas para identificar desviaciones respecto al comportamiento habitual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e639cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>src_user</th>\n",
       "      <th>total_events</th>\n",
       "      <th>failed_events</th>\n",
       "      <th>fail_ratio</th>\n",
       "      <th>dst_hosts</th>\n",
       "      <th>src_hosts</th>\n",
       "      <th>nbhours_events</th>\n",
       "      <th>nbhours_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>ANONYMOUS LOGON@C1065</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>434</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>U561@DOM1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>U556@DOM1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>U555@DOM1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-01 00:00:00</td>\n",
       "      <td>U553@DOM1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    dt               src_user  total_events  failed_events  \\\n",
       "0  1970-01-01 00:00:00  ANONYMOUS LOGON@C1065           434              0   \n",
       "1  1970-01-01 00:00:00              U561@DOM1            17              0   \n",
       "2  1970-01-01 00:00:00              U556@DOM1            20              0   \n",
       "3  1970-01-01 00:00:00              U555@DOM1            49              0   \n",
       "4  1970-01-01 00:00:00              U553@DOM1            30              0   \n",
       "\n",
       "   fail_ratio  dst_hosts  src_hosts  nbhours_events  nbhours_ratio  \n",
       "0         0.0          1         82             434            1.0  \n",
       "1         0.0          3          4              17            1.0  \n",
       "2         0.0          5          4              20            1.0  \n",
       "3         0.0          9         10              49            1.0  \n",
       "4         0.0          6          7              30            1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "DATA_PATH = \"lanl_db_feature.csv\"  # ajusta si tu fichero se llama distinto\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75339e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4195.788251876831)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ocupación en memoria del csv en MB\n",
    "df.memory_usage(deep=True).sum() / (1024**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2f6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (42866142, 9)\n"
     ]
    }
   ],
   "source": [
    "#Nos indica cuantos perfiles de comportamiento hay\n",
    "print(\"Shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e940082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt                0.0\n",
      "src_user          0.0\n",
      "total_events      0.0\n",
      "failed_events     0.0\n",
      "fail_ratio        0.0\n",
      "dst_hosts         0.0\n",
      "src_hosts         0.0\n",
      "nbhours_events    0.0\n",
      "nbhours_ratio     0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Se comprueban valores nulos y se ordenan las columnas que muestran las diez peores ventanas de comportamiento.\n",
    "print(df.isna().mean().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd122b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_events</th>\n",
       "      <th>failed_events</th>\n",
       "      <th>fail_ratio</th>\n",
       "      <th>dst_hosts</th>\n",
       "      <th>src_hosts</th>\n",
       "      <th>nbhours_events</th>\n",
       "      <th>nbhours_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.286614e+07</td>\n",
       "      <td>4.286614e+07</td>\n",
       "      <td>4.286614e+07</td>\n",
       "      <td>4.286614e+07</td>\n",
       "      <td>4.286614e+07</td>\n",
       "      <td>4.286614e+07</td>\n",
       "      <td>4.286614e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.444509e+01</td>\n",
       "      <td>2.980180e-01</td>\n",
       "      <td>1.314286e-02</td>\n",
       "      <td>2.900863e+00</td>\n",
       "      <td>3.393979e+00</td>\n",
       "      <td>6.157894e+00</td>\n",
       "      <td>2.396926e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.179786e+02</td>\n",
       "      <td>1.218741e+01</td>\n",
       "      <td>1.108429e-01</td>\n",
       "      <td>2.717707e+00</td>\n",
       "      <td>1.266600e+01</td>\n",
       "      <td>6.128290e+01</td>\n",
       "      <td>4.268958e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.100000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.807300e+04</td>\n",
       "      <td>4.608000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.751000e+03</td>\n",
       "      <td>2.026000e+03</td>\n",
       "      <td>1.371200e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total_events  failed_events    fail_ratio     dst_hosts     src_hosts  \\\n",
       "count  4.286614e+07   4.286614e+07  4.286614e+07  4.286614e+07  4.286614e+07   \n",
       "mean   2.444509e+01   2.980180e-01  1.314286e-02  2.900863e+00  3.393979e+00   \n",
       "std    1.179786e+02   1.218741e+01  1.108429e-01  2.717707e+00  1.266600e+01   \n",
       "min    2.000000e+00   0.000000e+00  0.000000e+00  1.000000e+00  1.000000e+00   \n",
       "25%    5.000000e+00   0.000000e+00  0.000000e+00  1.000000e+00  2.000000e+00   \n",
       "50%    1.100000e+01   0.000000e+00  0.000000e+00  2.000000e+00  2.000000e+00   \n",
       "75%    2.500000e+01   0.000000e+00  0.000000e+00  4.000000e+00  4.000000e+00   \n",
       "max    1.807300e+04   4.608000e+03  1.000000e+00  2.751000e+03  2.026000e+03   \n",
       "\n",
       "       nbhours_events  nbhours_ratio  \n",
       "count    4.286614e+07   4.286614e+07  \n",
       "mean     6.157894e+00   2.396926e-01  \n",
       "std      6.128290e+01   4.268958e-01  \n",
       "min      0.000000e+00   0.000000e+00  \n",
       "25%      0.000000e+00   0.000000e+00  \n",
       "50%      0.000000e+00   0.000000e+00  \n",
       "75%      0.000000e+00   0.000000e+00  \n",
       "max      1.371200e+04   1.000000e+00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se generan estadisticas más descriptivas para comprobar el estado del dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d5407",
   "metadata": {},
   "source": [
    "## Fase 1: Selección de variables\n",
    "\n",
    "Se van a seleccionar características numéricas que más representan el comportamiento IAM:\n",
    "volumen de actividad, fallos, diversidad de hosts y actividad fuera de horario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e639d88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seleccionamos las columnas con las que vamos a trabajar en el dataset, en este caso, hay columnas que no son necesarias para entrenar a nuestras IAs\n",
    "DB_COLS = [\n",
    "    \"total_events\",\n",
    "    \"failed_events\",\n",
    "    \"fail_ratio\",\n",
    "    \"dst_hosts\",\n",
    "    \"src_hosts\",\n",
    "    \"nbhours_events\",\n",
    "    \"nbhours_ratio\"\n",
    "]\n",
    "\n",
    "#Comprobamos que las columnas son las que se encuentran en el documento\n",
    "miss = [c for c in DB_COLS if c not in df.columns]\n",
    "miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab1d0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_events</th>\n",
       "      <th>failed_events</th>\n",
       "      <th>fail_ratio</th>\n",
       "      <th>dst_hosts</th>\n",
       "      <th>src_hosts</th>\n",
       "      <th>nbhours_events</th>\n",
       "      <th>nbhours_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>434</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_events  failed_events  fail_ratio  dst_hosts  src_hosts  \\\n",
       "0           434              0         0.0          1         82   \n",
       "1            17              0         0.0          3          4   \n",
       "2            20              0         0.0          5          4   \n",
       "3            49              0         0.0          9         10   \n",
       "4            30              0         0.0          6          7   \n",
       "\n",
       "   nbhours_events  nbhours_ratio  \n",
       "0             434            1.0  \n",
       "1              17            1.0  \n",
       "2              20            1.0  \n",
       "3              49            1.0  \n",
       "4              30            1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#última limpieza del dataset antes de empezar el entrenamiento\n",
    "\n",
    "# Se sustituyen valores infinitos por 0\n",
    "db = df[DB_COLS].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "db.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a86e84f",
   "metadata": {},
   "source": [
    "Dado que las variables presentan escalas y magnitudes muy diferentes, se va a aplicar una normalización previa al entrenamiento de los modelos. Este proceso evita que las características de mayor rango dominen el aprendizaje y permite que todas las variables contribuyan de forma equilibrada a la detección de accesos atípicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1efbf9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42866142, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalización del dataset para empezar a entrenar a los modelos de aprendizaje automático\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "db_scaled = scaler.fit_transform(db)\n",
    "db_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc1f7f",
   "metadata": {},
   "source": [
    "## Isolation Forest\n",
    "\n",
    "Modelo no supervisado que aísla observaciones raras mediante particiones aleatorias.\n",
    "Es adecuado para detección de outliers en grandes volúmenes y se utiliza cuando no existen etiquetas.\n",
    "está inspirado en el algoritmo de clasificación y regresión Random Forest.\n",
    "El modelo Isolation Forest está formado por la combinación de múltiples árboles llamados isolation trees. Estos árboles se crean de forma similar a los de clasificación-regresión: las observaciones de entrenamiento se van separando de forma recursiva creando las ramas del árbol hasta que cada observación queda aislada en un nodo terminal. Sin embargo, en los isolation tree, la selección de los puntos de división se hace de forma aleatoria. Aquellas observaciones con características distintas al resto, quedarán aisladas a las pocas divisiones, por lo que el número de nodos necesarios para llegar a esta observación desde el inicio del árbol (profundidad) es menor que para el resto.\n",
    "\n",
    "El modelo Isolation Forest se obtiene al combinar múltiples isolation tree, cada uno entrenado con una muestra distinta generada por bootstrapping a partir de los datos de originales. El valor predicho para cada observacion es el número de divisiones promedio que se han necesitado para aislar dicha observacion en el conjunto de árboles. Cuanto menor es este valor, mayor es la probabilidad de que se trate de una anomalía.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6506905f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF entrenado. Tiempo(s): 415.62\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# contamination = proporción esperada de anomalías (hiperparámetro)\n",
    "# Se va a empezar entre el rango de 0.01–0.05\n",
    "iso = IsolationForest(\n",
    "    n_estimators=200, #Número de árboles en el bosque\n",
    "    contamination=0.05, #Proporción esperada de anomalías \n",
    "    random_state=42, #Reproducibilidad\n",
    "    n_jobs=-1 #se usan todos los núcleos de CPu disponibles\n",
    ")\n",
    "\n",
    "iso.fit(db_scaled)\n",
    "\n",
    "t_train_if = time.time() - t0\n",
    "print(\"IF entrenado. Tiempo(s):\", round(t_train_if, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15bdc2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF scoring. Tiempo(s): 481.34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "iso_label\n",
       " 1    40722903\n",
       "-1     2143239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scoring\n",
    "t0 = time.time()\n",
    "\n",
    "# decision_function: mayor => más normal. Menor => más anómalo\n",
    "df[\"iso_score\"] = iso.decision_function(db_scaled)\n",
    "\n",
    "# predict: 1 normal, -1 anómalo\n",
    "df[\"iso_label\"] = iso.predict(db_scaled)\n",
    "\n",
    "t_score_if = time.time() - t0\n",
    "print(\"IF scoring. Tiempo(s):\", round(t_score_if, 2))\n",
    "\n",
    "df[[\"iso_score\", \"iso_label\"]].describe()\n",
    "df[\"iso_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cdd4d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest guardado: (42866142, 4)\n"
     ]
    }
   ],
   "source": [
    "#Se van a guardar los resultados del entrenamiento para poder evaluarlos posteriormente \n",
    "out_if = pd.DataFrame({\n",
    "    \"iso_score\": df[\"iso_score\"].values,\n",
    "    \"iso_label\": df[\"iso_label\"].values\n",
    "})\n",
    "\n",
    "# Se añade trazabilidad si existe (Quién y cuando)\n",
    "if \"dt\" in df.columns:\n",
    "    out_if[\"dt\"] = df[\"dt\"].values\n",
    "if \"src_user\" in df.columns:\n",
    "    out_if[\"src_user\"] = df[\"src_user\"].values\n",
    "\n",
    "out_if.to_parquet(\"isolation_forest.parquet\", index=False)\n",
    "print(\"Isolation Forest guardado:\", out_if.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf2a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadatos de IF guardados\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "run_if_info = {\n",
    "    \"model\": \"IsolationForest\",\n",
    "    \"features\": DB_COLS,\n",
    "    \"scaler\": \"StandardScaler\",\n",
    "    \"params\": {\n",
    "        \"n_estimators\": 200,\n",
    "        \"contamination\": 0.05,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"run_if_info.json\", \"w\") as f:\n",
    "    json.dump(run_if_info, f, indent=2)\n",
    "\n",
    "print(\"Metadatos de IF guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdc360",
   "metadata": {},
   "source": [
    "### Hipótesis — Isolation Forest\n",
    "\n",
    "Se ha entrenado Isolation Forest sobre perfiles horarios y se ha obtenido un score continuo de anomalía, permitiendo generar un ranking Top-K de accesos potencialmente atípicos para revisión.\n",
    "La distribución del score permite evaluar la presencia de una cola de anomalías, y el Top-K facilita el análisis cualitativo y la estimación de métricas operativas como Precision@K y tasa de falsos positivos mediante revisión manual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f25083",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "Autoencoders es un tipo de red neuronal que se utiliza para el aprendizaje no supervisado. Su función principal es aprender una representación comprimida de los datos de entrada. Consta de dos partes principales: el codificador y el decodificador.\n",
    "\n",
    "Codificador: Esta parte de la red comprime la entrada en una representación en espacio latente. Codifica los datos de entrada como una representación codificada (comprimida) en una dimensión reducida.\n",
    "Decodificador: El decodificador tiene como objetivo reconstruir los datos de entrada a partir de la representación codificada. Intenta generar una salida lo más cercana posible a la entrada original.\n",
    "\n",
    "La idea clave es que autoencoders se entrena para minimizar los errores de reconstrucción, lo que los hace eficientes en el aprendizaje de la distribución de los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1544f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42866142, 7), dtype('float32'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparación de los datos normalizados. Se usa float32 y mini-batches para evitar usar una memoria elevada.\n",
    "db_ae = db_scaled.astype(np.float32)\n",
    "db_ae.shape, db_ae.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "218f6683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#se importan librerías y se indica donde entrenar el modelo\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c6393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 42866142\n",
      "Features: 7\n",
      "Tamaño aprox float32: 1.12 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Filas:\", db_scaled.shape[0])\n",
    "print(\"Features:\", db_scaled.shape[1])\n",
    "\n",
    "# tamaño aproximado de X_ae en RAM si es float32\n",
    "approx_gb = db_scaled.shape[0] * db_scaled.shape[1] * 4 / (1024**3)\n",
    "print(f\"Tamaño aprox float32: {approx_gb:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4553d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el dataLoader para entregar los datos en lotes (batches)\n",
    "#Se usa Pytorch para crear este dataset para el modelo\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "dataset = TensorDataset(torch.from_numpy(db_ae))\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True, #se mezclan filas para que el modelo no vea patrones por orden temporal\n",
    "    drop_last=False #para no perder filas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff1f9e",
   "metadata": {},
   "source": [
    "Define un autoencoder:\n",
    "\n",
    "Encoder: comprime el vector de features.\n",
    "Decoder: intenta reconstruir el vector original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e35f93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=7, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Se define el modelo\n",
    "input_dim = db_ae.shape[1]\n",
    "latent_dim = max(2, input_dim // 2)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "ae = AutoEncoder(input_dim, latent_dim).to(device)\n",
    "ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da0cf29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - MSE: 0.122366\n",
      "Epoch 2/5 - MSE: 0.015145\n",
      "Epoch 3/5 - MSE: 0.009545\n",
      "Epoch 4/5 - MSE: 0.006947\n",
      "Epoch 5/5 - MSE: 0.006250\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento del modelos para minimizar el error de reconstrucción\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "ae.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for (batch,) in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon = ae(batch)\n",
    "        loss = loss_fn(recon, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "        n += batch.size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} - MSE: {total_loss/n:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ad8d237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4.286614e+07\n",
       "mean     5.941479e-03\n",
       "std      3.710887e-01\n",
       "min      8.723942e-05\n",
       "25%      2.492777e-04\n",
       "50%      4.954672e-04\n",
       "75%      9.488109e-04\n",
       "max      5.212789e+02\n",
       "Name: ae_score, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Proceso de scoring\n",
    "ae.eval()\n",
    "scores = np.empty(len(db_ae), dtype=np.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    start = 0\n",
    "    eval_loader = DataLoader(\n",
    "        TensorDataset(torch.from_numpy(db_ae)),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    for (batch,) in eval_loader:\n",
    "        batch = batch.to(device)\n",
    "        recon = ae(batch)\n",
    "        mse = ((recon - batch) ** 2).mean(dim=1).cpu().numpy()\n",
    "\n",
    "        end = start + len(mse)\n",
    "        scores[start:end] = mse\n",
    "        start = end\n",
    "\n",
    "df[\"ae_score\"] = scores\n",
    "df[\"ae_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f7192f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder guardado: (42866142, 3)\n"
     ]
    }
   ],
   "source": [
    "#Guardar resultado \n",
    "\n",
    "out_ae = pd.DataFrame({\n",
    "    \"ae_score\": df[\"ae_score\"].values\n",
    "})\n",
    "\n",
    "if \"dt\" in df.columns:\n",
    "    out_ae[\"dt\"] = df[\"dt\"].values\n",
    "if \"src_user\" in df.columns:\n",
    "    out_ae[\"src_user\"] = df[\"src_user\"].values\n",
    "\n",
    "out_ae.to_parquet(\"autoencoders.parquet\", index=False)\n",
    "print(\"Autoencoder guardado:\", out_ae.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13c5dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadatos del Autoencoder guardados\n"
     ]
    }
   ],
   "source": [
    "#Guardar metadatos del experimento\n",
    "import json\n",
    "\n",
    "run_ae_info = {\n",
    "    \"model\": \"Autoencoder\",\n",
    "    \"features\": DB_COLS,\n",
    "    \"scaler\": \"StandardScaler\",\n",
    "    \"params\": {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss\": \"MSE\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"run_ae_info.json\", \"w\") as f:\n",
    "    json.dump(run_ae_info, f, indent=2)\n",
    "\n",
    "print(\"Metadatos del Autoencoder guardados\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b38eb",
   "metadata": {},
   "source": [
    "## LOF\n",
    "Como este modelo se utiliza para una base de datos con un menor volumen, vamos a entrenarlos por separado con un muestreo sacado de la propia base de datos que se preparo en una etapa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a9decaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra: (1000000, 7) float32\n"
     ]
    }
   ],
   "source": [
    "#muestreo aleatorio reproducible.\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SAMPLE_N = 1_000_000\n",
    "\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "sample_idx = rng.choice(db_scaled.shape[0], size=SAMPLE_N, replace=False)\n",
    "\n",
    "db_muestra = db_scaled[sample_idx].astype(np.float32, copy=False)\n",
    "print(\"Muestra:\", db_muestra.shape, db_muestra.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dea8619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: sample_idx.parquet (1000000, 3)\n"
     ]
    }
   ],
   "source": [
    "sample_meta = pd.DataFrame({\"row_id\": sample_idx})\n",
    "\n",
    "# trazabilidad opcional si existe en df\n",
    "for c in [\"dt\", \"src_user\"]:\n",
    "    if c in df.columns:\n",
    "        sample_meta[c] = df.loc[sample_idx, c].values\n",
    "\n",
    "sample_meta.to_parquet(\"sample_idx.parquet\", index=False)\n",
    "print(\"Guardado: sample_idx.parquet\", sample_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d2655da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOF entrenado. Tiempo(s): 213.46\n"
     ]
    }
   ],
   "source": [
    "#LOF “normal” (novelty=False) está pensado para fit_predict en el mismo set, pero para tener un score usable y consistente, mejor novelty=True.\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "LOF_NEIGHBORS = 35\n",
    "LOF_CONTAM = 0.02\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=LOF_NEIGHBORS,\n",
    "    contamination=LOF_CONTAM,\n",
    "    novelty=True,     \n",
    "    n_jobs=-1\n",
    ")\n",
    "lof.fit(db_muestra)\n",
    "\n",
    "t_train_lof = time.time() - t0\n",
    "print(\"LOF entrenado. Tiempo(s):\", round(t_train_lof, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c69733cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOF scoring. Tiempo(s): 423.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 1    981525\n",
       "-1     18475\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scoring\n",
    "t0 = time.time()\n",
    "\n",
    "lof_score_raw = lof.decision_function(db_muestra)  # mayor => más normal\n",
    "lof_score = -lof_score_raw                  # mayor => más anómalo\n",
    "lof_label = lof.predict(db_muestra)                # 1 normal, -1 anómalo\n",
    "\n",
    "t_score_lof = time.time() - t0\n",
    "print(\"LOF scoring. Tiempo(s):\", round(t_score_lof, 2))\n",
    "\n",
    "pd.Series(lof_label).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62e6c18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: lof.parquet (1000000, 5)\n"
     ]
    }
   ],
   "source": [
    "#guardar datos\n",
    "out_lof = sample_meta.copy()\n",
    "out_lof[\"lof_score\"] = lof_score.astype(np.float32)\n",
    "out_lof[\"lof_label\"] = lof_label.astype(np.int8)\n",
    "\n",
    "out_lof.to_parquet(\"lof.parquet\", index=False)\n",
    "print(\"Guardado: lof.parquet\", out_lof.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ab41b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: run_lof_info.json\n"
     ]
    }
   ],
   "source": [
    "#Guardar metadatos\n",
    "import json\n",
    "run_sample_info = {\n",
    "    \"sample\": {\"n\": int(SAMPLE_N), \"random_state\": int(RANDOM_STATE)},\n",
    "    \"features\": DB_COLS,\n",
    "    \"scaler\": \"StandardScaler\",\n",
    "    \"lof\": {\n",
    "        \"n_neighbors\": int(LOF_NEIGHBORS),\n",
    "        \"contamination\": float(LOF_CONTAM),\n",
    "        \"novelty\": True,\n",
    "        \"train_seconds\": float(t_train_lof),\n",
    "        \"score_seconds\": float(t_score_lof)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"run_lof_info.json\", \"w\") as f:\n",
    "    json.dump(run_sample_info, f, indent=2)\n",
    "\n",
    "print(\"Guardado: run_lof_info.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
